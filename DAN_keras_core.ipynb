{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading data from google cloud"
      ],
      "metadata": {
        "id": "IR7K14_Ov9YI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyHKa3bNhty5",
        "outputId": "18be82ec-7136-4ea3-f5cc-0fee1ced6620"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-24 17:29:19--  https://storage.googleapis.com/kerascvnlp_data/young-affectnet-hq.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.253.117.128, 142.250.99.128, 142.250.107.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.253.117.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5441294496 (5.1G) [application/zip]\n",
            "Saving to: ‘young-affectnet-hq.zip’\n",
            "\n",
            "young-affectnet-hq. 100%[===================>]   5.07G   177MB/s    in 46s     \n",
            "\n",
            "2023-07-24 17:30:06 (112 MB/s) - ‘young-affectnet-hq.zip’ saved [5441294496/5441294496]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://storage.googleapis.com/kerascvnlp_data/young-affectnet-hq.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CRW5ufU3Qsj"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/young-affectnet-hq.zip -d data/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## installing and importing required libraries"
      ],
      "metadata": {
        "id": "mZlQ6Ir3wCA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q keras-core"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOtfVr7a9Hf6",
        "outputId": "4bf1838e-9232-42ac-afba-61b1aa29f661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/753.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/753.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m583.7/753.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m753.1/753.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the [DAN model ](https://arxiv.org/pdf/2109.07270.pdf)"
      ],
      "metadata": {
        "id": "-ZhCIAYowH-h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALbPisTtWxPr",
        "outputId": "3457e7f8-db47-435e-8ad1-0c4c5eab2333"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 8)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from keras_core import Model, Sequential\n",
        "from keras_core.layers import Layer\n",
        "import keras_core.layers as nn\n",
        "import keras_core as keras\n",
        "\n",
        "class ChannelAttn(Layer):\n",
        "    def __init__(self, c=512) -> None:\n",
        "        super(ChannelAttn,self).__init__()\n",
        "        self.gap = nn.AveragePooling2D(7)\n",
        "        self.attention = Sequential([\n",
        "            nn.Dense(32),\n",
        "            nn.BatchNormalization(),\n",
        "            nn.ReLU(),\n",
        "            nn.Dense(c,activation='sigmoid')]\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "\n",
        "        x = self.gap(x)\n",
        "        x = nn.Flatten()(x)\n",
        "        y = self.attention(x)\n",
        "        return x * y\n",
        "\n",
        "\n",
        "class SpatialAttn(Layer):\n",
        "    def __init__(self, c=512):\n",
        "        super(SpatialAttn,self).__init__()\n",
        "        self.conv1x1 = Sequential([\n",
        "            nn.Conv2D(256, 1),\n",
        "            nn.BatchNormalization()]\n",
        "        )\n",
        "        self.conv_3x3 = Sequential([\n",
        "            nn.ZeroPadding2D(padding=(1, 1)),\n",
        "            nn.Conv2D(512, 3,1),\n",
        "            nn.BatchNormalization()]\n",
        "        )\n",
        "        self.conv_1x3 = Sequential([\n",
        "            nn.ZeroPadding2D(padding=(0, 1)),\n",
        "            nn.Conv2D(512, (1,3)),\n",
        "            nn.BatchNormalization()]\n",
        "        )\n",
        "        self.conv_3x1 = Sequential([\n",
        "            nn.ZeroPadding2D(padding=(1, 0)),\n",
        "            nn.Conv2D(512,(3,1)),\n",
        "            nn.BatchNormalization()]\n",
        "        )\n",
        "        self.norm = nn.ReLU()\n",
        "\n",
        "    def call(self, x) :\n",
        "        y = self.conv1x1(x)\n",
        "        y = self.norm(self.conv_3x3(y) + self.conv_1x3(y) + self.conv_3x1(y))\n",
        "        y = tf.math.reduce_sum(y,axis=1, keepdims=True)\n",
        "        return x*y\n",
        "\n",
        "\n",
        "class CrossAttnHead(Layer):\n",
        "    def __init__(self, c=512):\n",
        "        super(CrossAttnHead,self).__init__()\n",
        "        self.sa = SpatialAttn(c)\n",
        "        self.ca = ChannelAttn(c)\n",
        "\n",
        "    def call(self, x):\n",
        "        return self.ca(self.sa(x))\n",
        "\n",
        "\n",
        "@keras.saving.register_keras_serializable(package='custom')\n",
        "class DAN(Model):\n",
        "    def __init__(self, num_classes=8,trainable=True,dtype='float32'):\n",
        "        super(DAN,self).__init__()\n",
        "        self.mod = keras.applications.ResNet50(\n",
        "            include_top=False,\n",
        "            weights=\"imagenet\",\n",
        "            input_shape=(224,224,3)\n",
        "        )\n",
        "        self.mod.trainable= False\n",
        "        self.num_head = 4\n",
        "        self.hd = CrossAttnHead()\n",
        "        self.hd=[]\n",
        "        for i in range(self.num_head):\n",
        "          self.hd.append(CrossAttnHead())\n",
        "        self.features = nn.Conv2D(512, 1,padding='same')\n",
        "        self.fc = nn.Dense(num_classes)\n",
        "        self.bn = nn.BatchNormalization()\n",
        "\n",
        "    def call(self, x) :\n",
        "        x = self.mod(x)\n",
        "        x=self.features(x)\n",
        "        heads = []\n",
        "        for h in self.hd:\n",
        "            heads.append(h(x))\n",
        "\n",
        "        heads = tf.transpose(tf.stack(heads),perm=(1,0,2))\n",
        "        heads = keras.ops.log_softmax(heads, axis=1)\n",
        "        out = self.bn(self.fc(tf.math.reduce_sum(heads,axis=1)))\n",
        "        return out\n",
        "\n",
        "model = DAN()\n",
        "img = tf.random.normal(shape=[10, 224, 224, 3])\n",
        "preds = model(img)\n",
        "print(preds.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "creating the image dataloader using the keras-core utils function"
      ],
      "metadata": {
        "id": "wcVEXZpmwR1P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-lfM07X386v",
        "outputId": "813829ba-e96d-4f0b-c49b-31d4a36da66a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 14648 files belonging to 8 classes.\n"
          ]
        }
      ],
      "source": [
        "train_ds = keras.utils.image_dataset_from_directory(\n",
        "            directory=\"data/\",\n",
        "            labels='inferred',\n",
        "            label_mode='categorical',\n",
        "            batch_size=32,\n",
        "            image_size=(224, 224))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-fYX5bf-Qhz"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',loss=keras.losses.CategoricalCrossentropy())\n",
        "model.fit(train_ds,epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving and loading the model using inbuilt functions of keras_core"
      ],
      "metadata": {
        "id": "rYr1Xx99vt3R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKxEstbn-zud"
      },
      "outputs": [],
      "source": [
        "model.save('weights.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYqcWuqkOJW7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3715156-4564-456f-fc66-8038c62cda0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras_core/src/saving/saving_lib.py:338: UserWarning: Skipping variable loading for optimizer 'adam', because it has 190 variables whereas the saved optimizer has 2 variables. \n",
            "  trackable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        }
      ],
      "source": [
        "pb = keras.saving.load_model('weights.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2Kb428rOgga",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ea79cca-e201-4e4f-be22-320af4af4105"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([10, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "pred = pb(img)\n",
        "pred.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzObYUiROqnJ"
      },
      "outputs": [],
      "source": [
        "!zip -r wgts.zip weights.keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVUFuilbizMP"
      },
      "outputs": [],
      "source": [
        "!wget https://storage.googleapis.com/kerascvnlp_data/danwgts/wgts.zip\n",
        "!unzip wgts.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgLYD0Kd1Z_X",
        "outputId": "c963fe41-60f0-434b-e783-c2ea720236b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tflite-support\n",
            "  Downloading tflite-support-0.1.0a1.tar.gz (390 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.3/390.3 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.4 (from tflite-support)\n",
            "  Using cached pybind11-2.10.4-py3-none-any.whl (222 kB)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tflite-support) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tflite-support) (1.22.4)\n",
            "Building wheels for collected packages: tflite-support\n",
            "  Building wheel for tflite-support (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflite-support: filename=tflite_support-0.1.0a1-cp310-cp310-linux_x86_64.whl size=5942401 sha256=c7e9b35b7492f34e69eee330b1b65e327b281488484e256ff208e30908d0ddbb\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/5c/da/9e5e661ec26e03ee57e69428d40fffbefe3c0aff649c55776d\n",
            "Successfully built tflite-support\n",
            "Installing collected packages: pybind11, tflite-support\n",
            "Successfully installed pybind11-2.10.4 tflite-support-0.1.0a1\n"
          ]
        }
      ],
      "source": [
        "!pip install tflite-support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ODbDxBn2hKV",
        "outputId": "8c1e5577-b1ad-445d-f3fe-61b8c70b5431"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-05-26 16:26:33.200042: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Finished populating metadata and associated file to the model:\n",
            "model.tflite\n",
            "The metadata json file has been saved to:\n",
            "/content/oiut/model.json\n",
            "The associated file that has been been packed to the model is:\n",
            "['labels.txt']\n"
          ]
        }
      ],
      "source": [
        "!python ./metadata_writer_for_image_classifier.py \\\n",
        "    --model_file=model.tflite \\\n",
        "    --label_file=labels.txt \\\n",
        "    --export_directory=/content/oiut"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrDY-KX8Pify",
        "outputId": "606c229d-fb65-47b8-9e32-a4920cb7c273"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_41_layer_call_fn, conv2d_41_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, dense_21_layer_call_fn, dense_21_layer_call_and_return_conditional_losses while saving (showing 5 of 99). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Convert the model\n",
        "pb = keras.saving.load_model('weights.keras')\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(pb) # path to the SavedModel directory\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model.\n",
        "with open('model.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "[Distract Your Attention: Multi-head Cross Attention Network for Facial Expression Recognition](https://arxiv.org/pdf/2109.07270.pdf)\n",
        "\n",
        "[Pytorch Implementation of DAN](https://github.com/yaoing/DAN)\n",
        "\n",
        "[Official Keras Core Documentation](https://keras.io/keras_core/)"
      ],
      "metadata": {
        "id": "m68bfzJJ8gVS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZnkiAi6O81D1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}